
도커 커밋 / 푸시
#docker commit -a "코멘트" 컨테이너이름 저장소/이름:버전
docker commit -a "glue spark jupyter notebook" glue_master megazonecloud/dsc_glue_dev:2.0
docker push megazonecloud/dsc_glue_dev:2.0

스파크 클러스터 스탠드얼론
docker network create -d bridge spark-bridge-network
브릿지 네트워크로 생성
docker run -it -d -p 8096:8080 -p 4096:4040 -p 4116:4041 --network spark-bridge-network -v ~/:/mnt --name spark_master kimhoon0312/glue_dev:1.3 /bin/bash
docker run -it -d -p 8097:8080 -p 4097:4040 -p 4117:4041 --network spark-bridge-network -v ~/:/mnt --name spark_node_2 kimhoon0312/glue_dev:1.3 /bin/bash
docker run -it -d -p 8098:8080 -p 4098:4040 -p 4118:4041 --network spark-bridge-network -v ~/:/mnt --name spark_node_3 kimhoon0312/glue_dev:1.3 /bin/bash

Host 네트워크로 생성
docker run -it -d --network host -v d:/mnt:/mnt --name spark_master kimhoon0312/glue_dev:1.3 /bin/bash
docker run -it -d --network host -v d:/mnt:/mnt --name spark_node_2 kimhoon0312/glue_dev:1.3 /bin/bash
docker run -it -d --network host -v d:/mnt:/mnt --name spark_node_3 kimhoon0312/glue_dev:1.3 /bin/bash
docker run -it -d --network host -v d:/mnt:/mnt --name spark_submit kimhoon0312/glue_dev:1.3 /bin/bash

docker start spark_master
docker start spark_node_2
docker start spark_node_3
docker start spark_submit

docker exec -it spark_master /bin/bash
docker exec -it spark_node_2 /bin/bash
docker exec -it spark_node_3 /bin/bash
docker exec -it spark_submit /bin/bash

#노드의 IP 주소 확인
docker network ls
docker network inspect bridge spark-bridge-network

#마스터노드 실행
/root/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/sbin/start-master.sh --port 4041
#워커노드에서 실행
/root/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/sbin/start-slave.sh -c 2 -m 2G spark://0.0.0.0:4041

#작업제출
/glue/bin/gluesparksubmit --master spark://0.0.0.0:4041 --driver-memory 2g --executor-memory 2g --num-executors 2 --executor-cores 2 test_glue_job.py

#노드 종료
/root/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/sbin/stop-master.sh
/root/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/sbin/stop-slave.sh

#샘플 코드

import sys
import json
import boto3
from awsglue.gluetypes import *
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
from pyspark.sql import Row
from pyspark.sql.types import *

try:
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
except Exception as ex:
    print('Exception : '+str(ex))
    glueContext = GlueContext(spark.sparkContext)

print('test start')
df = spark.read.csv('/mnt/table_mapping.csv')
df.show(10)
repartitiondf = df.coalesce(6)
repartitiondf.write.parquet('/mnt/coalescetest_6')
